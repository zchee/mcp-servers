{
  "id": "10611",
  "year": "2020",
  "url": "https://developer.apple.com/videos/play/wwdc2020/10611/",
  "title": "Explore ARKit 4",
  "speakers": [],
  "duration": "",
  "topics": [
    "Graphics & Games"
  ],
  "hasTranscript": true,
  "hasCode": true,
  "transcript": {
    "fullText": "♪ Voiceover: Hello, and welcome to WWDC.\n\nQuinton: Hi, my name's Quinton, and I'm an engineer on the ARKit team.\n\nToday both Praveen and I get to show you some of the new features in ARKit with iOS 14.\n\nSo let's jump right in and explore ARKit 4.\n\nThis release adds many advancements to ARKit which already powers the world's largest AR platform, iOS.\n\nARKit gives you the tools to create AR experiences that change the way your users see the world.\n\nSome of these tools include device motion tracking, camera scene capture, and advanced scene processing which all help to simplify the task of building a realistic and immersive AR experience.\n\nLet's see what's next with ARKit.\n\nSo first, we're going to take a look at the location anchor API.\n\nLocation anchors bring your AR experience onto the global scale by allowing you to position virtual content in relation to the globe.\n\nThen we'll see whatt he new LiDAR sensor brings to ARKit with scene geometry.\n\nScene geometry provides apps with a mesh of the surrounding environment that can be used for everything from occlusion to lighting.\n\nNext we'll look at the technology that enables scene geometry, the depth API.\n\nWe're opening up this API to give apps access to a dense depth map to enable new possibilities using the LiDAR sensor.\n\nAnd additionally, the LiDAR sensor improves object placement.\n\nWe'll go over some best practices to make sure your apps take full advantage of the newest object-placement techniques.\n\nAnd we'll wrap up with some improvements to FaceTracking.\n\nLet's start with location anchors.\n\nBefore we get too far, let's look at how we got to this point.\n\nARKit started on iOS with the best tracking.\n\nNo QR codes. No external equipment needed.\n\nJust start an AR experience by placing content around you.\n\nThen we added multi-user experiences.\n\nYour AR content could then be shared with a friend using a separate device to make experiences social.\n\nAnd last year we brought people into ARKit.\n\nAR experiences are now aware of the people on the scene.\n\nMotion capture is possible with just a single iOS device, and people occlusion makes AR content even more immersive as people can walk right in front of a virtual object.\n\nAll these features combine to make some amazing experiences.\n\nBut what's next? So now we're bringing AR into the outdoors with location anchors.\n\nLocation anchors enable you to place AR content in relation to the globe.\n\nThis means you can now place virtual objects and create AR experiences by specifying a latitude, longitude, and altitude.\n\nARKit will take your geographic coordinates as well as high-resolution map data from Apple Maps to place your AR experiences at the specific world location.\n\nThis whole process is called visual localization, and it will precisely locate your device in relation to the surrounding environment more accurately than could be done before with just GPS.\n\nAll this is possible due to advance machine learning techniques running right on your device.\n\nThere's no processing in the cloud and no images sent back to Apple.\n\nARKit also takes care of merging the local coordinate system to the geographic coordinate system, so you can work in one unified system, regardless of how you want to create your AR experiences.\n\nTo access these features we've added a new configuration, ARGeoTrackingConfiguration and ARGeoAnchors are what you'll use to place in content the same way as other ARKit anchors.\n\nLet's see some location anchors in action.\n\nWe've got a video here in front of the Ferry Building in San Francisco.\n\nYou can see a large virtual sculpture.\n\nThat's actually the companion sculpture created by KAWS, and viewed in the acute art app.\n\nSince it was placed with location anchors, everyone who uses the app at the Ferry Building can enjoy the virtual art in the same place and the same way.\n\nLet's see what's under the hood in ARKit to make this all work.\n\nSo when using geo tracking, we download all the detailed map data from Apple Maps around your current location.\n\nPart of this data is a localization map that contains feature points of the surrounding area that can be seen from the street.\n\nThen with the localization map, your current location, and images from your device, we can use advanced machine learning to visually localize and determine your device's position.\n\nAll this is happening under the hood in ARKit to give you a precise, globally-aware pose without worrying about any of this complexity.\n\nThe location anchor API can be broken down into three main parts.\n\nARGeoTrackingConfiguration is the configuration that you'll use to take advantage of all of the new location anchor features.\n\nThis configuration contains a subset of the world-tracking features that are compatible with geo tracking.\n\nThen once you've started an AR session with the geo-tracking configuration, you'll be able to create ARGeoAnchors just like any other ARKit anchor.\n\nAnd also while using geo tracking, there's a new tracking status that's important to monitor.\n\nThis is contained in ARGeoTrackingStatus and provides valuable feedback to improve the geo-tracking experience.\n\nSo building an app with location anchors can be broken down into a few steps.\n\nThe first is checking availability of geo tracking.\n\nARGeoTrackingConfiguration has a few methods that let us check the preconditions using the rest of the location anchor API.\n\nThen location anchors can be added once you know there's full geo-tracking support.\n\nAnd after anchors are added, we can use a rendering engine to place virtual content.\n\nWe'll then need take care of geo-tracking transitions.\n\nOnce started, geo tracking will move through a few states that may need some user intervention to ensure the best geo-tracking experience.\n\nSo let's build a simple point of interest app to see what these steps look like in practice.\n\nIn our app, we're going to start with helping our users find the iconic Ferry Building in San Francisco, California.\n\nAs you can see, we've placed a sign to make the building easy to spot.\n\nTo begin the app, let's first start with checking availability.\n\nAs with many ARKit features, we need to make sure the current device is supported before attempting to start an experience.\n\nLocation anchors are available on devices with an A12 bionic chip and newer, as well as GPS.\n\nARGeoTrackingConfigurations isSupported class method should be used to check for the support.\n\nFor geo tracking, we also need to check if the current location is supported.\n\nWe need to be in a location that has all the required maps data to localize.\n\nThe geo tracking configuration has a method to check your current location support as well as an arbitrary latitude and longitude.\n\nAdditionally, once a geo tracking session is started, ARKit will ask a user for permission for both camera and location.\n\nARKit has always asked for camera permission, but location permission is needed to do geo tracking.\n\nLet's see what this looks like in code.\n\nARGeoTrackingConfiguration has all the class methods that we need to check before starting our AR session.\n\nWe'll first check if the current device is supported with \"isSupported,\" and then we'll check if our current location is available for geo tracking with \"checkAvailability.\" If this check fails, we'll get an error with more info to display to the user.\n\nFor example, if the user hasn't given the app location permissions.\n\nThen once we know our current device and location are supported, we can go ahead and start the session.\n\nSince we're using RealityKit, we'll need our ARView and then update the configuration.\n\nBy default, ARView uses a world-tracking configuration, and so we need to pass in a GeoTrackingConfiguration when running the session.\n\nThe next step is adding a location anchor.\n\nTo do this, we'll use the new ARAnchor subclass ARGeoAnchor.\n\ngeoAnchors are similar to existing ARKit anchors in many ways.\n\nHowever, because geoAnchors operate on global coordinates, we can't create them with just transforms.\n\nWe need to specify their geographic coordinates with latitude, longitude, and altitude.\n\nThe most common way to create geoAnchors will be through specifying just latitude and longitude, which this allows ARKit to fill in the altitude based on maps data of the ground-level.\n\nLet's now add a location anchor to our point of interest app.\n\nSo for our app, we need to start by finding the Ferry Building's location.\n\nOne way we can get the latitude and longitude is through the maps app.\n\nWhen we place the marker in the maps app, we now get up to six digits of precision after the decimal.\n\nIt's important to use six or more digits so that we get a precise location to place our contents.\n\nOnce we have a latitude and longitude, we can make a geoAnchor.\n\nWe don't need to specify an altitude because we'll let ARKit use maps data to determine the elevation of the ground level.\n\nThen we'll add the geoAnchor to our session.\n\nAnd since we're using RealityKit to render our virtual content, and we've already created our geoAnchor, we can go ahead and attach the anchor to an entity to mark the Ferry Building.\n\nLet's run our app and see what it looks like.\n\nWe'll start near the Ferry Building in San Francisco looking towards Market Street.\n\nAnd as we pan around, we can see some of the palm trees that line the city.\n\nAnd soon the Ferry Building will come into view.\n\nOur sign looks to be on the ground, which is expected, but the text is rotated.\n\nSince we'd like to find the Ferry Building easily from a distance, we'd really like to have the sign floating a few meters in the air and facing towards the city.\n\nSo how do we do this? Well, to position this content, we need to first look at the coordinate's system of the geoAnchor.\n\ngeoAnchors are fixed to cardinal directions.\n\nTheir axes are set when you create the anchor, and this orientation will remain unchanged for the rest of the session.\n\nA geoAnchors X-axis is always pointed east, and the Z-axis is always pointed south for any geographic coordinate.\n\nSince we're using a right-handed coordinate system, this leaves positive y pointing up away from the ground.\n\ngeoAnchors, like all other ARKit anchors are immutable.\n\nThis means we'll need to use our rendering engine to rotate or translate our virtual objects from the geoAnchors origin.\n\nLet's clean up our sign that we placed in front of the Ferry Building.\n\nSo here's some RealityKit code to start updating our sign.\n\nAfter getting the signEntity and adding it to the geoAnchor entity, we want to rotate the sign towards the city.\n\nTo do this, we'll rotate it by a little less than 90 degrees clockwise and we'll elevate the sign's position by 35 meters.\n\nBoth of these operations are in relation to the geoAnchor entity that we had previously created.\n\nLet's see what this looks like in the app.\n\nNow when we pan around and we get to our Ferry Building, our sign is high in the air, and we can see it from a distance.\n\nThe text is much easier to read in this orientation.\n\nThis looks great, but we're missing some crucial information here about the geo-tracking state that we can use to guide the user to the best geo-tracking experience.\n\nWhen using a GeoTrackingConfiguration, there's a new GeoTrackingStatus object that's available on ARFrame and ARsession observer.\n\nARGeoTrackingStatus encapsulates all the current state information of geo tracking, similar to the world-tracking information that's available on ARCamera.\n\nWithin geo tracking status, there is a state.\n\nThis state indicates how far along geo tracking is during localization.\n\nThere's also a property that provides more information about the current localization state called geo Tracking State Reason, and there's an accuracy provided once geo tracking localizes.\n\nLet's take a closer look at the geo tracking state.\n\nWhen an AR session begins, geoTrackingState starts at initializing.\n\nAt this point, geo tracking is waiting for the world tracking to initialize.\n\nFrom initializing, the tracking state can immediately go to not available if geo tracking isn't supported in the current location.\n\nIf you're using the checkAvailability class method on geoTrackingConfiguration, you should rarely get into the state.\n\nOnce geo tracking moves to localizing, ARKit is receiving images as well as maps data and is trying to compute both.\n\nHowever, during both the initializing and localizing states, there could be issues detected that prevent localization.\n\nThese issues are communicated through geoTrackingStateReason.\n\nThis reason should be used to inform the user how to help geo tracking localize.\n\nSome possible reasons include the device is pointed too low, which would then inform the user to raise the device, or geoDataNotLoaded, and we'd inform the user that a network connection is required.\n\nFor all possible reasons, have a look at ARGeoTrackingTypes.h.\n\nIn general we want to encourage users to point their devices at buildings and other stationary structures that are visible from the street.\n\nParking lots, open fields, and other environments that dynamically change have a lower chance of localizing.\n\nAfter addressing any geoTrackingStateReasons, geo tracking should become localized.\n\nIt's at this point that you should start your AR experience.\n\nIf you place objects before localization, the objects could jump to unintended locations.\n\nAdditionally once localized ARGeoTrackingAccuracy is provided to help you gauge what experiences should be enabled.\n\nIt's also important to always monitor geo tracking state as it's possible for geo tracking to move back to localizing or even initializing such as when tracking is lost or map data isn't available.\n\nLet's take a look at how we can add this tracking state to improve our sample app.\n\nNow we can see this whole time we were actually localizing when looking at Market Street and the surrounding buildings.\n\nAs we pan around, we can see from the tracking state that we localize and then the accuracy increases to high.\n\nI think we've got our app just about ready, at least for the Ferry Building.\n\nSo we've added a more expansive location anchor sample project on developer.apple.com that I encourage you to check out after this talk.\n\nFor more information on the RealityKit features used, check out last year's talk introducing RealityKit and Reality Composer.\n\nIn our sample app, we saw how to create location anchors by directly specifying coordinates.\n\nWe already knew the geographic coordinates for the Ferry Building.\n\nHowever, these coordinates could have come from any source, such as our app bundle, our web backend, or really any database.\n\nAnother way to create a location anchor is via user interaction.\n\nWe could expand on our app in the future by allowing users to tap the screen to save their own point of interest.\n\ngetGeoLocation(for point) on ARSession allows us to get geographic coordinates from any world point in ARKit coordinate space.\n\nFor example, this could have come from a raycast, or location on a plane.\n\nLocation anchors are available for you today with iOS14, and we're starting with support in the San Francisco Bay Area, New York, Los Angeles, Chicago, and Miami, with more cities coming through the summer.\n\nAll iPhones and iPads with an A12 bionic chip or newer, as well as GPS, are supported.\n\nAlso, for any apps that require location anchors exclusively, you can use device capability keys to limit your app in the App Store to only compatible hardware.\n\nIn addition to the GPS key, you'll need to use the new key for devices with an A12 bionic chip or newer that is available in iOS14.\n\nSo with location anchors, you can now bring your AR experiences onto the global scale.\n\nWe went over how ARGeoTrackingConfiguration is the entry point to adding location anchors to your app.\n\nWe saw how to add ARGeoAnchors to your ARScene and how to position content in relation to those anchors.\n\nWe also saw how ARGeoTrackingStatus can be used to help guide the user to the best geo-tracking experience.\n\nAnd now here's Praveen to tell you more about scene geometry.\n\nPraveen Gowda: Hi everyone.\n\nI'm Praveen Gowda. I'm an engineer on the ARKit team.\n\nToday I'm going to take you through some of the APIs available in iOS14 that help bring the power of the LiDAR scanner to your applications.\n\nIn ARKit 3.5, we introduced the scene geometry API powered by the LiDAR scanner on the new iPad Pro.\n\nBefore we go into scene geometry, let's take a look at how the LiDAR scanner works.\n\nThe LiDAR shoots light onto the surroundings and then collects the light reflected off the surfaces in the scene.\n\nThe depth is estimated by measuring the time it took for the light to go from the LiDAR to the environment and reflect back to the scanner.\n\nAnd this entire process runs millions of times every second.\n\nThe LiDAR scanner is used by the same geometry API to provide a topological map of the environment.\n\nThis can be optionally fused with semantic classification which enables apps to recognize and classify physical objects.\n\nThis provides an opportunity for creating richer AR experiences where apps can now upload virtual objects with the real world or use physics to enable realistic interactions between virtual and physical objects.\n\nOr to use virtual lighting on real world surfaces and in many other use cases that we were to imagine.\n\nLet's take a quick look at scene geometry in action.\n\nHere is a living room and once the scene geometry API is turned on the entire visible room is meshed.\n\nTriangles vary in size to show the optimum detail for each surface.\n\nThe color mesh appears once semantic classification is enabled.\n\nEach color represents a different classification such as blue for the seats and green for the floor.\n\nAs we saw, the scene geometry feature is built by leveraging the depth data gathered from the LiDAR scanner.\n\nIn iOS14, we have a new ARKit depth API that provides access to the same depth data.\n\nThe API provides a dense depth image where a pixel in the image corresponds to depth in meters from the camera.\n\nWhat we see here is a debug visualization of this depth where there's a gradient from blue to red, where blue represents regions closer to the camera and red represents those far away.\n\nThe depth data would be available at 60 Hz, associated with each AR frame.\n\nThe scene geometry feature is built on top of this API where depth data across multiple frames are aggregated and processed to construct a 3D mesh.\n\nThis API is powered by the LiDAR scanner and is available on devices which have LiDAR.\n\nHere is an illustration of how the depth map is generated.\n\nThe colored RGB image from the wide-angle camera and the depth ratings from the LiDAR scanner are fused together using advanced machine learning algorithms to create a dense depth map that is exposed through the API.\n\nThis operation runs at 60 times per second with the depth map available on every AR frame.\n\nTo access the depth data, each AR frame will have a new property called sceneDepth.\n\nThis provides an object of type, ARDepthData.\n\nARDepthData is a container for two buffers.\n\nOne is a depthMap and the other is a confidenceMap.\n\nThe depthMap is a CV pixel buffer but each pixel represents depth and is in meters, and this depth corresponds to the distance from plane of the camera to a point in the world.\n\nOne thing to note is that the depth map is smaller in resolution compared to the captured image on the AR frame which still presents the same aspect ratio.\n\nThe other buffer on the ARDepthData object is the confidenceMap.\n\nSince the measurement of depth using LiDAR is based on the light which reflects from objects, the accuracy of the depth map can be impacted by the nature of the surrounding environment.\n\nChallenging surfaces, such as those which are highly reflective or those with high absorption, can lower the accuracy of the depth.\n\nThis accuracy is expressed through a value we call confidence.\n\nFor each depth pixel, there is a corresponding confidence value of type ARConfidenceLevel, and this value can either be low, medium, or high and will help to filter depth based on the requirements of your application.\n\nLet's see how we can use the depth API.\n\nI begin with creating an ARSession and an ARTrackingConfiguration.\n\nThere is a new frame semantic called sceneDepth, which allows you to turn on the depth API.\n\nAs always, I check if the frameSemantic is supported on the device using the supportsFrameSemantics method on the configuration class.\n\nThen we can set the frameSemantic to sceneDepth and run the configuration.\n\nAfter this, I can access the depth data from the sceneDepth property on ARFrame using the didUpdate frame delegate method.\n\nAdditionally if you have an AR app that uses people occlusion feature, and then search the personSegmentationWithDepth frameSemantic, then you will automatically get sceneDepth on devices that support the sceneDepth frameSemantic with no additional power cost to your application.\n\nHere is a demo of an app that we built using the depth API.\n\nThe depth from the depthMap is unprojected to 3D to form a point cloud.\n\nThe point cloud is colored using the captured image on the ARFrame.\n\nBy accumulating depth data across multiple AR frames, you get a dense 3D point cloud like the one we see here.\n\nI can also filter the point clouds based on the confidence level.\n\nThis is the point cloud formed by all the depth pixels including those with low confidence.\n\nAnd here is the point cloud while filtering depth with confidence is medium or high.\n\nAnd this is the point cloud we get by using only that depth which has high confidence.\n\nThis gives us a clear picture of how the physical properties of surfaces can impact the confidence level of its depth.\n\nYour application and its tolerance to inaccuracies in depth will determine how you will filter the depth based on its confidence level.\n\nLet's take a closer look at how we built this app.\n\nFor each ARFrame we access, the sceneDepth property with the ARDepth data objects, providing us with the depth and the confidenceMap.\n\nThe key part of the app is a metal vertex shader called unproject.\n\nAs the name suggests, it unprojects the depth data from the depth map to the 3D space using parameters on the ARCamera such as the cameras transform, it's intrinsics, and the projection matrix.\n\nThe shader also uses captured image to sample color for each depth pixel.\n\nWhat we get as an output of this is the 3D point cloud which is then rendered using Metal.\n\nTo summarize, we have a new depth API in ARKit 4 which gives a highly accurate representation of the world.\n\nThere is a frame semantic called sceneDepth which allows you to enable the feature.\n\nOnce enabled, the depth data will be available at 60 Hz on each AR frame.\n\nThe depth data will have a depthMap and a confidenceMap, and the API is supported on devices with the LiDAR scanner.\n\nOne of the fundamental tasks in many AR apps is placing objects, and in ARKit 3, we introduced the raycasting API to make object placement easier.\n\nIn ARKit 4, The LiDAR scanner brings some great implements to raycasting.\n\nRaycasting is highly optimized for object placement and makes it easy to precisely place virtual objects in your AR app.\n\nPlacing objects in ARKit 4 is more precise and quicker, thanks to the LiDAR scanner.\n\nYour apps that already use raycasting will automatically benefit on a LiDAR-enabled device.\n\nRaycasting also leverages scene depth or scene geometry when available to instantly place objects in AR.\n\nThis works great even on featureless offices such as white walls.\n\nIn iOS14, the raycast API is recommended over hit-testing for object placement.\n\nBefore you start raycasting, you will need to create a raycast query.\n\nA raycast query describes the direction and the behavior of the ray used for raycasting.\n\nIt is composed of a raycast target which describes the type of surface that a ray can intersect with.\n\nExisting planes correspond to planes detected by ARKit, while considering the shape and size of the plane.\n\nInfinite planes are the same planes but with the shape and size ignored.\n\nAnd estimated planes are planes of arbitrary orientation formed from the feature points around the surface.\n\nThe raycasttarget alignment specifies the alignment of surfaces that a ray can intersect with.\n\nThis can be horizontal, vkertical, or any.\n\nThere are two types of raycasts.\n\nThere are single-shot raycasts which return a one-time result.\n\nAnd then that tracked raycasts which continuously update the results as ARKit's understanding of the world evolves.\n\nIn order to get the latest features object placement we are recommending migrating to the raycasting API as we deprecate hit-testing.\n\nThe code we see on the top is extracted from a sample app which uses hit-testing to place objects.\n\nIt performs a test with three different kinds of hit-test options.\n\nAnd it is usually followed by some custom heuristics to filter those results and figure out where to place the object.\n\nAll of that can be replaced with the few lines of raycasting code like the one we see below and ARKit will do the heavy lifting under the hood to make sure that your virtual objects always stay at the right place.\n\nRaycasting makes it easier than ever before to precisely place virtual objects in your ARKit applications.\n\nLet's move over to FaceTracking.\n\nFaceTracking allows you to detect faces in your front camera AR experience, overlay virtual content on them, and animate facial expressions in real time.\n\nThis is supported on all devices with the TrueDepth camera.\n\nNow with ARKit 4, FaceTracking support is extended to devices without a TrueDepth camera, as long as they have an Apple A12 bionic processor or later.\n\nThis includes the devices without the TrueDepth camera such as the new iPhone SE.\n\nElements of FaceTracking, such as face anchors, face geometry, and blendshapes will be available on all supported devices but capture depth data will be limited to devices with the TrueDepth camera.\n\nAnd that is ARKit 4.\n\nWith location anchors, you can now bring your AR experiences onto the global scale.\n\nAnd we looked at how we can use the LiDAR to build rich AR apps using the same geometry and the depth API.\n\nThere are exciting improvements in raycasting to make object placement in AR easier than ever before.\n\nAnd finally, FaceTracking is now supported on a wider range of devices.\n\nThank you.\n\nAnd we can't wait to check out all the great apps that you will\nbuild using ARKit 4.",
    "segments": []
  },
  "codeExamples": [
    {
      "timestamp": "6:58",
      "title": "Availability",
      "language": "swift",
      "code": "// Check device support for geo-tracking\nguard ARGeoTrackingConfiguration.isSupported else {\n    // Geo-tracking not supported on this device\n    return\n}\n\n// Check current location is supported for geo-tracking\nARGeoTrackingConfiguration.checkAvailability { (available, error) in\n    guard available else {\n        // Geo-tracking not supported at current location\n        return\n    }\n    // Run ARSession\n    let arView = ARView()\n    arView.session.run(ARGeoTrackingConfiguration())\n}"
    },
    {
      "timestamp": "8:38",
      "title": "Adding Location Anchors",
      "language": "swift",
      "code": "// Create coordinates\nlet coordinate = CLLocationCoordinate2D(latitude: 37.795313, longitude: -122.393792)\n\n// Create Location Anchor\nlet geoAnchor = ARGeoAnchor(name: \"Ferry Building\", coordinate: coordinate)\n\n// Add Location Anchor to session\narView.session.add(anchor: geoAnchor)\n\n// Create a RealityKit anchor entity \nlet geoAnchorEntity = AnchorEntity(anchor: geoAnchor)\n\n// Anchor content under the RealityKit anchor\ngeoAnchorEntity.addChild(generateSignEntity())\n\n// Add the RealityKit anchor to the scene\narView.scene.addAnchor(geoAnchorEntity)"
    },
    {
      "timestamp": "10:32",
      "title": "Positioning Content",
      "language": "swift",
      "code": "// Create a new entity for our virtual content\nlet signEntity = generateSignEntity();\n\n// Add the virtual content entity to the Geo Anchor entity\ngeoAnchorEntity.addChild(signEntity)\n\n// Rotate text to face the city\nlet orientation = simd_quatf.init(angle: -Float.pi / 3.5, axis: SIMD3<Float>(0, 1, 0))\nsignEntity.setOrientation(orientation, relativeTo: geoAnchorEntity)\n\n// Elevate text to 35 meters above ground level\nlet position = SIMD3<Float>(0, 35, 0)\nsignEntity.setPosition(position, relativeTo: geoAnchorEntity)"
    },
    {
      "timestamp": "14:08",
      "title": "User Interactive Location Anchors",
      "language": "swift",
      "code": "let session = ARSession()\nlet worldPosition = raycastLocationFromUserTap()\nsession.getGeoLocation(forPoint: worldPosition) { (location, altitude, error) in\n    if let error = error {\n        ...\n    }\n    let geoAnchor = ARGeoAnchor(coordinate: location, altitude: altitude)\n}"
    },
    {
      "timestamp": "20:32",
      "title": "Enabling the Depth API",
      "language": "swift",
      "code": "// Enabling the depth API\n\nlet session = ARSession()\nlet configuration = ARWorldTrackingConfiguration()\n\n// Check if configuration and device supports .sceneDepth\nif type(of: configuration).supportsFrameSemantics(.sceneDepth) {\n    // Activate sceneDepth\n    configuration.frameSemantics = .sceneDepth\n}\nsession.run(configuration)\n\n...\n\n// Accessing depth data\nfunc session(_ session: ARSession, didUpdate frame: ARFrame) {\n    guard let depthData = frame.sceneDepth else { return }\n    // Use depth data\n}"
    },
    {
      "timestamp": "21:12",
      "title": "Depth API alongside person occlusion",
      "language": "swift",
      "code": "// Using the depth API alongside person occlusion\n\nlet session = ARSession()\nlet configuration = ARWorldTrackingConfiguration()\n\n// Set required frame semantics\nlet semantics: ARConfiguration.FrameSemantics = .personSegmentationWithDepth\n        \n// Check if configuration and device supports the required semantics\nif type(of: configuration).supportsFrameSemantics(semantics) {\n    // Activate .personSegmentationWithDepth\n    configuration.frameSemantics = semantics\n}\nsession.run(configuration)"
    },
    {
      "timestamp": "25:41",
      "title": "Raycasting",
      "language": "swift",
      "code": "let session = ARSession()\nhitTest(point, types: [.existingPlaneUsingGeometry,\n                       .estimatedVerticalPlane,\n                       .estimatedHorizontalPlane])\n\nlet query = arView.makeRaycastQuery(from: point,\n                                    allowing: .estimatedPlane,\n                                    alignment: .any)\n\nlet raycast = session.trackedRaycast(query) { results in\n   // result updates\n}"
    }
  ],
  "resources": {
    "resourceLinks": [
      {
        "title": "Design",
        "url": "https://developer.apple.com/design/"
      },
      {
        "title": "ARKit",
        "url": "https://developer.apple.com/documentation/ARKit"
      },
      {
        "title": "Creating a fog effect using scene depth",
        "url": "https://developer.apple.com/documentation/ARKit/creating-a-fog-effect-using-scene-depth"
      },
      {
        "title": "Displaying a point cloud using scene depth",
        "url": "https://developer.apple.com/documentation/ARKit/displaying-a-point-cloud-using-scene-depth"
      },
      {
        "title": "Tracking geographic locations in AR",
        "url": "https://developer.apple.com/documentation/ARKit/tracking-geographic-locations-in-ar"
      },
      {
        "title": "Documentation",
        "url": "https://developer.apple.com/documentation/"
      },
      {
        "title": "Forums",
        "url": "https://developer.apple.com/forums/"
      },
      {
        "title": "Apple Design Awards",
        "url": "https://developer.apple.com/design/awards/"
      }
    ],
    "hdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10611/8/203AC69C-0F17-4709-B622-08C2740C7539/wwdc2020_10611_hd.mp4?dl=1",
    "sdVideo": "https://devstreaming-cdn.apple.com/videos/wwdc/2020/10611/8/203AC69C-0F17-4709-B622-08C2740C7539/wwdc2020_10611_sd.mp4?dl=1"
  },
  "relatedVideos": [
    {
      "id": "10073",
      "year": "2021",
      "title": "Explore ARKit 5",
      "url": "https://developer.apple.com/videos/play/wwdc2021/10073"
    },
    {
      "id": "10621",
      "year": "2020",
      "title": "Support performance-intensive apps and games",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10621"
    },
    {
      "id": "10601",
      "year": "2020",
      "title": "The artist’s AR toolkit",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10601"
    },
    {
      "id": "10612",
      "year": "2020",
      "title": "What's new in RealityKit",
      "url": "https://developer.apple.com/videos/play/wwdc2020/10612"
    },
    {
      "id": "604",
      "year": "2019",
      "title": "Introducing ARKit 3",
      "url": "https://developer.apple.com/videos/play/wwdc2019/604"
    },
    {
      "id": "603",
      "year": "2019",
      "title": "Introducing RealityKit and Reality Composer",
      "url": "https://developer.apple.com/videos/play/wwdc2019/603"
    }
  ],
  "extractedAt": "2025-07-18T10:34:30.544Z"
}